---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
title: "MRU: Analyses of Liver Enzyme data"
#thanks: "**Corresponding author**: rooijm@fsw.leidenuniv.nl"
author:
- name: Mark de Rooij
  affiliation: Leiden University, Methodology and Statistics Unit
abstract: ""
keywords: "Odds ratio, probability, visualization, ternary plot, trioscale"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
spacing: double
bibliography: ~/surfdrive/predictive-psychometrics/paper/predpsycho.bib
biblio-style: apsr
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/surfdrive/multldm/mrmdu")
library(nnet)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(GGally, quietly = TRUE, warn.conflicts = FALSE)
library(mvtnorm)
library(lmdu)
source("~/surfdrive/Shared/MRU/examples/mru.extended.R")
library(ggpubr)
library(metR)
library(tidyverse)
library(RColorBrewer)
library(jtools)
```

# Data

Plomteux showed that good separation between acute viral hepatitis (AVH, 57 patients), persistent chronic hepatitis (PCH, 44 patients), aggressive chronic hepatitis (ACH, 40 patients) and post-necrotic cirrhosis (PNN, 77 patients) could be achieved with only three liver function test. 

The three analytes, aspartate aminotransferase (AST), alanine aminotransferase (ALT) and glutamate dehydrogenase (GIDH), were used to construct a four-group logistic model. 

```{r}
load("~/surfdrive/MultinomialBook/Datasets/liverdata/liverdata.RData")
colnames(liverdat) = c("Disease", "AS", "AL", "GD", "X?")
X = liverdat[ ,-c(1,5)]
xnames= colnames(X)
X.df = data.frame(X)
y = liverdat[ , 1]
y = factor(y, levels = c("1","2" ,"3", "4"), labels = c("AVH","PCH","ACH","PNC"))
G = class.ind(y)
colnames(G) = c("AVH","PCH","ACH","PNC")
G = G[, c(4,1,2,3)]
#ggpairs(X.df, aes(color=factor(y)))
```

The data seems to be very skew. Let us try a transformation down the ladder of powers:

```{r}
XX = log(X)
X.df = data.frame(XX)
#ggpairs(X.df, aes(color=factor(y)))
X = scale(XX, center = TRUE, scale = FALSE)
```

This looks much better. Therefore, in the remainder we will work with these log-transformed predictor variables. 

Lesaffre and Albert: "In a full parametric approach such as normal discriminant analysis, the first action would be to transform the data to remove skewness. In a semiparametric model, such as the logistic model, it is debatable whether to apply the transformation on the marginal distributions"

```{r}
rm(X.df, XX, liverdat)
```

# Investigating Local optima

```{r}
set.seed(1234)

M1results = vector(mode = "list", length = 100)
M2results = vector(mode = "list", length = 100)
M3results = vector(mode = "list", length = 100)

M1results[[1]] = mru.da(X, G, m = 1)
M2results[[1]] = mru.da(X, G, m = 2)
M3results[[1]] = mru.da(X, G, m = 3)

for(r in 2:100){
  M1results[[r]] = mru.random( X, G, m = 1 )
  M2results[[r]] = mru.random( X, G, m = 2 )
  M3results[[r]] = mru.random( X, G, m = 3 )
}
save(M1results, M2results, M3results, file = "liverresults.Rdata")
```

```{r}
library(ggplot2)
dev1 = rep(NA, 100)
dev2 = rep(NA, 100)
dev3 = rep(NA, 100)
for(r in 1:100){
  dev1[r] = M1results[[r]]$deviance
  dev2[r] = M2results[[r]]$deviance
  dev3[r] = M3results[[r]]$deviance
}
df = data.frame(repl = rep(1:100, 3), dimensionality = as.factor(rep(1:3, each = 100)), deviance = c(dev1, dev2, dev3))

p = ggplot(df, aes(dimensionality, deviance)) + geom_boxplot() + theme_apa()
ggsave("~/surfdrive/multldm/mrmdu/paper/figures/liverresults.pdf", plot = p)
p
```

# Analysis with Distance Model


```{r}

out.liver1 = M1results[[which.min(dev1)]]
out.liver2 = M2results[[which.min(dev2)]]
out.liver3 = M3results[[which.min(dev3)]]

fit = matrix(NA, 3, 3)
fit[ , 1] = c(1,2,3)
fit[1, 2] = out.liver1$deviance
fit[2, 2] = out.liver2$deviance
fit[3, 2] = out.liver3$deviance
fit[1, 3] = ncol(X) * 1 + ncol(G) * 1 
fit[2, 3] = ncol(X) * 2 + ncol(G) * 2 - 2*1/2 
fit[3, 3] = ncol(X) * 3 + ncol(G) * 3 - 3*2/2
colnames(fit) = c("Dimensionality", "Deviance", "#params")
fit
```

Let us have a closer look at the two dimensional solution. 

```{r, echo = FALSE}
U = X %*% out.liver2$B
V = out.liver2$V
B = out.liver2$B

B
V
Xo = X
P = nrow(B)
  
# for solid line
MCx1 <- data.frame(labs=character(),
                   varx = integer(),
                   Dim1 = double(),
                   Dim2 = double(), stringsAsFactors=FALSE)
# for markers
MCx2 <- data.frame(labs=character(),
                   varx = integer(),
                   Dim1 = double(),
                   Dim2 = double(), stringsAsFactors=FALSE)

ll = 0
lll = 0
for(pp in 1:P){
  b = matrix(B[pp , ], 2, 1)
  # solid line
  minx = min(Xo[, pp])
  maxx = max(Xo[, pp])
  m.x1 = c(minx,maxx)
  markers1 = matrix(m.x1, 2, 1) 
  markerscoord1 = outer(markers1, b) # markers1 %*% t(b %*% solve(t(b) %*% b))
  MCx1[(ll + 1): (ll + 2), 1] = paste0(c("min", "max"), pp)
  MCx1[(ll + 1): (ll + 2), 2] = pp
  MCx1[(ll + 1): (ll + 2), 3:4] = markerscoord1 
  ll = ll + 2
  # markers
  m.x2 = pretty(Xo[, pp]) 
  m.x2 = m.x2[which(m.x2 > minx & m.x2 < maxx)]
  l.m = length(m.x2)
  markers2 = matrix(m.x2, l.m, 1) 
  markerscoord2 = outer(markers2, b) # markers2 %*% t(b %*% solve(t(b) %*% b))
  MCx2[(lll + 1): (lll + l.m), 1] = paste(m.x2) 
  MCx2[(lll + 1): (lll + l.m), 2] = pp
  MCx2[(lll + 1): (lll + l.m), 3:4] = markerscoord2 
  lll = lll + l.m
} # loop p
  


NN = as.data.frame(U)
colnames(NN) = c("Dim1", "Dim2")

VV = as.data.frame(V)  
colnames(VV) = c("Dim1", "Dim2")

p = ggplot() +
    geom_point(data = VV, aes(x = Dim1, y = Dim2), colour = "darkgreen", size = 3) +
    geom_point(data = NN, aes(x = Dim1, y = Dim2, color = y), size = 1, show.legend = FALSE) + 
    xlab("Dimension 1") + 
    ylab("Dimension 2") + 
    #xlim(-12,12) + ylim(-12,12) +
    coord_fixed()   

p = p + geom_text(data = VV, aes(x = Dim1, y = Dim2), 
                  label = colnames(G),
                  vjust = 0, nudge_y = -0.5
                  )

######################################################
# variable axes with ticks and markers for predictors
######################################################
xcol = "lightskyblue"
p = p + geom_abline(intercept = 0, slope = B[,2]/B[,1], colour = xcol, linetype = 3) +
  geom_line(data = MCx1, aes(x = Dim1, y = Dim2, group = varx), col = xcol, size = 1) + 
  geom_point(data = MCx2, aes(x = Dim1, y = Dim2), col = xcol) + 
  geom_text(data = MCx2, aes(x = Dim1, y = Dim2, label = labs), nudge_y = -0.08, size = 1.5)

a = ceiling(max(abs(c(ggplot_build(p)$layout$panel_scales_x[[1]]$range$range, ggplot_build(p)$layout$panel_scales_y[[1]]$range$range))))

idx1 = apply(abs(B), 1, which.max)
t = s = rep(NA,P)
for(pp in 1:P){
    t[(pp)] = (a * 1.1)/(abs(B[pp,idx1[(pp)]])) * B[pp,-idx1[(pp)]]
    s[(pp)] = sign(B[pp,idx1[(pp)]])
}
CC = cbind(idx1, t, s)
bottom = which(CC[, "idx1"] == 2 & CC[, "s"] == -1)
top =  which(CC[, "idx1"] == 2 & CC[, "s"] == 1)
right = which(CC[, "idx1"] == 1 & CC[, "s"] == 1)
left = which(CC[, "idx1"] == 1 & CC[, "s"] == -1)

p = p + scale_x_continuous(limits = c(-a,a), breaks = CC[bottom, "t"], labels = xnames[bottom], 
                             sec.axis = sec_axis(trans ~ ., breaks = CC[top, "t"], labels = xnames[top]))
p = p + scale_y_continuous(limits = c(-a,a), breaks = CC[left, "t"], labels = xnames[left], 
                             sec.axis = sec_axis(trans ~ ., breaks = CC[right, "t"], labels = xnames[right]))  

p = p + theme_bw() + coord_fixed()

p = p + theme(axis.line = element_line(colour = "black"),
                    panel.grid.major = element_blank(),
                    panel.grid.minor = element_blank(),
                    panel.border = element_blank(),
                    panel.background = element_blank()) 

p
ggsave("~/surfdrive/multldm/mrmdu/paper/figures/liverplot1.pdf", plot = p, width = 11.7, height = 8.3, units = "in", limitsize = FALSE)
```

Let us have a further look at the classification performance
```{r}
D = outer(diag(U %*% t(U)), rep(1, 4)) + outer(rep(1,218), diag(V %*% t(V))) - 2* U %*% t(V)
D = sqrt(D)
PR = exp(-D)/rowSums(exp(-D))
yhat = apply(PR, 1, which.max)
yy = apply(G, 1, which.max)
tab1 = table(yy, yhat)
tab1
```

In total, `r 100*sum(diag(tab1))/sum(tab1)` percent is correctly classified 

# Simulation

First develop a function that generates data based on population class points, regression weights and some covariance matrix:

```{r}
gendata = function(N, covX, B, V){
  library(mvtnorm)
  library(poLCA)
  P = nrow(B)
  C = nrow(V)
  X = rmvnorm(N, mean = rep(0, P), sigma = covX)
  X = scale(X, center = TRUE, scale = FALSE)
  U = X %*% B
  
  D = sqrt(outer(diag(U %*% t(U)), rep(1, C)) + outer(rep(1, N), diag(V %*% t(V))) - 2 * U %*% t(V))
  Pr = exp(-D)/rowSums(exp(-D))
  G = class.ind(rmulti(Pr))
  output = list(X = X, G = G)
}

mse = function(A, B){(A-B)^2}

```

And now we simulate data with these parameters and varying sample sizes (100, 200, 500, 1000). On the generated data sets we fit the multinomial restricted unfolding and we compare the obtained results with the population parameters.

```{r}
V = out.liver2$V
B = out.liver2$B
covX = cov(X)

# small simulation
Ns = c(100, 200, 500, 1000)
plts = vector(mode = "list", length = length(Ns))
Bbias = vector(mode = "list", length = length(Ns))
Brmse = vector(mode = "list", length = length(Ns))
Vbias = vector(mode = "list", length = length(Ns))
Vrmse = vector(mode = "list", length = length(Ns))

source("ggbagplot.R")
set.seed(1234)
for(n in 1:length(Ns)){
  N = Ns[n]
  Bs = vector(mode = "list", length = 100)
  Vs = vector(mode = "list", length = 100)
  
  
  for(rep in 1:100){
    #cat("This is repetition:", rep, "\n")
    mydat = gendata(N, covX, B, V)
    out <- mru.user(mydat$X, mydat$G, m = 2, B.start = B, V.start = V)
    # orthogonal procrustes analysis on U
    pq = svd(t(V) %*% out$V)
    TT = pq$v %*% t(pq$u)
    Bs[[rep]] = out$B %*% TT
    Vs[[rep]] = out$V %*% TT
  }
  
  # # bias
  Bbias[[n]] = Reduce("+", Bs) / length(Bs) - B
  Vbias[[n]] = Reduce("+", Vs) / length(Vs) - V
  # 
  # rmse
  Brmse[[n]] = sqrt(Reduce("+" ,lapply(Bs, mse, B))/length(Bs))
  Vrmse[[n]] = sqrt(Reduce("+" ,lapply(Vs, mse, V))/length(Vs))
  
  ####################################################
  ####################################################
  # A visualization of the results
  ####################################################
  ####################################################
  
  Bdf = data.frame(B); colnames(Bdf) = c("x", "y")
  Vdf = data.frame(V); colnames(Vdf) = c("x", "y")
  Vdf$class = as.factor(c(1,2,3,4))
  
  # class points
  Vslong = matrix(NA, 400, 2); Bslong = matrix(NA, 300, 2)
  for(r in 1:100){
    Vslong[((r-1)*4 + 1):(r*4), ] = Vs[[r]]
    Bslong[((r-1)*3 + 1):(r*3), ] = Bs[[r]]
  }

  Vslong = data.frame(cbind(rep(1:4, 100), Vslong))
  colnames(Vslong) = c("class", "x", "y")
  Vslong$class = as.factor(Vslong$class)

  # hull_data <-
  #   Vslong %>%
  #   group_by(class) %>%
  #   slice(chull(x, y))

  # plt = ggplot(Vslong, aes(x = x, y = y, col = class)) +
  #   # xlim(-15,15) + 
  #   # ylim(-15,15) +
  #   geom_point(alpha = 0.5) + 
  #   geom_polygon(data = hull_data, aes(fill = class, colour = class), alpha = 0.3, show.legend = FALSE) + 
  #   scale_color_manual(values = brewer.pal(9,"Greens")[6:9]) +
  #   scale_fill_manual(values = brewer.pal(9,"Greens")[6:9]) 
  
  plt = ggplot(Vslong, aes(x = x, y = y, col = class)) +
    geom_point(alpha = 0.5) + 
    geom_bag(prop = 0.9, aes(fill = class, colour = class), alpha = 0.3, show.legend = FALSE) +
    scale_color_manual(values = brewer.pal(8,"Paired")[c(2,4,6,8)]) +
    scale_fill_manual(values = brewer.pal(8,"Paired")[c(2,4,6,8)]) +
    theme(legend.position = "none")
  
  # predictor variables
  Bslong = data.frame(cbind(rep(1:3, 100), Bslong))
  colnames(Bslong) = c("pred", "x", "y")
  Bslong$pred = as.factor(Bslong$pred)
  
  plt = plt + 
    # geom_abline(intercept = 0, slope = Bslong$y/Bslong$x, col = "lightskyblue", alpha = 0.3) + 
    geom_point(data = Bslong, aes(x = x, y = y), colour = "darkblue", alpha = 0.3) 
    # geom_bag(data = Bslong, prop = 0.9, aes(fill = pred, colour = pred), alpha = 0.3, show.legend = FALSE) +
    # scale_color_manual(values = brewer.pal(9,"Blues")[c(5,6,7)]) +
    # scale_fill_manual(values = brewer.pal(9,"Blues")[c(5,6,7)]) +
    # theme(legend.position = "none")

  plt = plt + 
    geom_point(data = Vdf, aes(x = x, y = y), colour = brewer.pal(8,"Paired")[c(2,4,6,8)], shape = 18, size = 5)+
    geom_abline(intercept = 0, slope = Bdf[ ,2]/Bdf[ ,1], colour = "darkblue", size = 1) + 
    geom_point(data = Bdf, aes(x =x, y = y), col = "darkblue", size = 5)

  plt = plt + 
    labs(
      x = "Dimension 1",
      y = "Dimension 2"
      ) + 
    xlim(-15,15) + 
    ylim(-15,15) +
    coord_fixed() +
    theme_apa() + 
    theme(legend.position = "none") 

  # for(r in 1:100){
  #   Bdfr = data.frame(Bs[[r]]); colnames(Bdfr) = c("x", "y")
  #   Vdfr = data.frame(Vs[[r]]); colnames(Vdfr) = c("x", "y")
  #   plt = plt + geom_point(data = Bdfr, aes(x = x, y = y), col = "darkblue", size = 0.2, alpha = 0.1) + 
  #     geom_point(data = Vdfr, aes(x = x,y = y), col = "darkred", size = 0.2, alpha = 0.1)
  # }
  
  plts[[n]] = plt + labs(title = paste("Estimates for N = ", N))
}

Bbias
Brmse

Vbias
Vrmse

save(Bbias, Brmse, Vbias, Vrmse, file = "simliverresults.Rdata")
plt = ggarrange(plts[[1]], plts[[2]], plts[[3]], plts[[4]], nrow = 2, ncol = 2)  
plt

ggsave("~/surfdrive/multldm/mrmdu/paper/figures/simliver.pdf", plot = plt, width = 11.7, height = 8.3, units = "in", limitsize = FALSE)
```

# Analysis with Squared Distance Model

Identification restrictions

- Translation: set the coordinates for first class to zero
- Rotation: set the upper triangle of \bf{V} to zero
- Scaling: set some elements in \bf{V} to one

The following function minimizes the deviance for the IPC model. 

```{r, echo = FALSE}
ipc.deviance <- function(pars, G, X){
  # Deviance of IPC model for LIVER data
  # pars: initial values of parameters
  # G: indicator matrix representing the response classes(n  by J)
  # X: predictor variables (n by p + 1); should include a vector of ones
  # CALL: stats <- optim(pars, ipc.deviance2, NULL, Y, X, Z, method ="BFGS", 
  #                      control = list(trace = 2, maxit = 100), hessian = F)
  # Change to hessian = T if se's need to be computed.
  # \copyright M. de Rooij, 23-5-2013                 \
  # ------------------------------------------------------------------
  # extract matrix B from the pars
  I = nrow(G)
  C = ncol(G)
  p = ncol(X)
  B = matrix(pars[1:(2*p)], p, 2)
  # create Z - matrix with coordinates of class points
  # TRANSLATION, SCALING, AND ROTATION
  V = matrix(0, C, 2)
  V[2,1] = V[3,2] = 1
  V[3:C,1] = pars[(2*p+1):(2*p+2)]
  V[4:C,2] = pars[(2*p+3)]

  # Make the linear predictors and compute the squred distances
  N = X %*% B
  
  v2 = rowSums(V ^ 2)
  D =  - 2 * N %*% t(V) + rep(1, I) %o% v2  
  
  # Compute probabilities and deviance
  P = exp(-D)
  sp = rowSums(P)
  P = (1 / sp) * P
  dev = -2 * sum(G * log(P))
}
```

With the following code we first create our design matrix and initiate parameters for minimization of the deviance function. Then we call the \texttt{optim}-function to find a set of parameters that maximizes the likelihood or minimizes the deviance. 

```{r}
X = cbind(1,X)
pars0 = c(rep(0,8), rnorm(3))
out = optim(pars0,ipc.deviance, G = G, X = X, method = "BFGS", control = list(trace = 2, maxit = 100), hessian = F)
```

The value of the deviance is `r out$value` for which the estimated parameters are
```{r}
P = ncol(X)
C = ncol(G)

B = matrix(out$par[1:(2*P)], P, 2)
B

U = X %*% B

V = matrix(0, C, 2)
V[2,1] = V[3,2] = 1
V[3:C,1] = out$par[(2*P+1):(2*P+2)]
V[4:C,2] = out$par[(2*P+3)]
V

```


Let us have a further look at the classification performance
```{r}
D2 = outer(diag(U %*% t(U)), rep(1, 4)) + outer(rep(1,218), diag(V %*% t(V))) - 2* U %*% t(V)
PR2 = exp(-D2)/rowSums(exp(-D2))
yhat2 = apply(PR2, 1, which.max)
tab2 = table(yy, yhat2)
tab2
```

In total, `r 100*sum(diag(tab2))/sum(tab2)` percent is correctly classified 


## Graphical representation

```{r, echo = FALSE}
# translation such that U is centered
VV = V - outer(rep(1,4), B[1, ])
# rotation to principal axes
UU = X[, -1] %*% B[-1, ]
R = eigen(t(UU)%*%UU)$vectors
B = B[-1, ] %*% R
UU = UU %*% R
VV = VV %*% R
```

Let us have a further look at the classification performance
```{r}
D2 = outer(diag(UU %*% t(UU)), rep(1, 4)) + outer(rep(1,218), diag(VV %*% t(VV))) - 2* UU %*% t(VV)
PR2 = exp(-D2)/rowSums(exp(-D2))
yhat2 = apply(PR2, 1, which.max)
tab2 = table(yy, yhat2)
tab2
```

In total, `r 100*sum(diag(tab2))/sum(tab2)` percent is correctly classified 


```{r}
NN = as.data.frame(UU)
colnames(NN) = c("Dim1", "Dim2")

VV = as.data.frame(V)  
colnames(VV) = c("Dim1", "Dim2")

P = nrow(B)
Xo = X[ , -1]

# for solid line
MCx1 <- data.frame(labs=character(),
                   varx = integer(),
                   Dim1 = double(),
                   Dim2 = double(), stringsAsFactors=FALSE)
# for markers
MCx2 <- data.frame(labs=character(),
                   varx = integer(),
                   Dim1 = double(),
                   Dim2 = double(), stringsAsFactors=FALSE)

ll = 0
lll = 0
for(pp in 1:P){
  b = matrix(B[pp , ], 2, 1)
  # solid line
  minx = min(Xo[, pp])
  maxx = max(Xo[, pp])
  m.x1 = c(minx,maxx)
  markers1 = matrix(m.x1, 2, 1) 
  markerscoord1 = outer(markers1, b) # markers1 %*% t(b %*% solve(t(b) %*% b))
  MCx1[(ll + 1): (ll + 2), 1] = paste0(c("min", "max"), pp)
  MCx1[(ll + 1): (ll + 2), 2] = pp
  MCx1[(ll + 1): (ll + 2), 3:4] = markerscoord1 
  ll = ll + 2
  # markers
  m.x2 = pretty(Xo[, pp]) 
  m.x2 = m.x2[which(m.x2 > minx & m.x2 < maxx)]
  l.m = length(m.x2)
  markers2 = matrix(m.x2, l.m, 1) 
  markerscoord2 = outer(markers2, b) # markers2 %*% t(b %*% solve(t(b) %*% b))
  MCx2[(lll + 1): (lll + l.m), 1] = paste(m.x2) 
  MCx2[(lll + 1): (lll + l.m), 2] = pp
  MCx2[(lll + 1): (lll + l.m), 3:4] = markerscoord2 
  lll = lll + l.m
} # loop p
  
p2 = ggplot() +
    geom_point(data = VV, aes(x = Dim1, y = Dim2), colour = "darkgreen", size = 3) +
    geom_point(data = NN, aes(x = Dim1, y = Dim2, color = y), size = 1, show.legend = FALSE) + 
    xlab("Dimension 1") + 
    ylab("Dimension 2") 

p2 = p2 + geom_text(data = VV, aes(x = Dim1, y = Dim2), 
                  label = colnames(G),
                  vjust = 0, nudge_y = -0.5)


xcol = "lightskyblue"
p2 = p2 + geom_abline(intercept = 0, slope = B[,2]/B[,1], colour = xcol, linetype = 3) +
    geom_line(data = MCx1, aes(x = Dim1, y = Dim2, group = varx), col = xcol, size = 1) + 
    geom_point(data = MCx2, aes(x = Dim1, y = Dim2), col = xcol) + 
    geom_text(data = MCx2, aes(x = Dim1, y = Dim2, label = labs), nudge_y = -0.08, size = 1.5)

a = ceiling(max(abs(c(ggplot_build(p2)$layout$panel_scales_x[[1]]$range$range, ggplot_build(p2)$layout$panel_scales_y[[1]]$range$range))))

idx1 = apply(abs(B), 1, which.max)
t = s = rep(NA,(P))
for(pp in 1:(P)){
    t[(pp)] = (a * 1.1)/(abs(B[pp,idx1[(pp)]])) * B[pp,-idx1[(pp)]]
    s[(pp)] = sign(B[pp,idx1[(pp)]])
}
CC = cbind(idx1, t, s)
bottom = which(CC[, "idx1"] == 2 & CC[, "s"] == -1)
top =  which(CC[, "idx1"] == 2 & CC[, "s"] == 1)
right = which(CC[, "idx1"] == 1 & CC[, "s"] == 1)
left = which(CC[, "idx1"] == 1 & CC[, "s"] == -1)


p2 = p2 + scale_x_continuous(limits = c(-a,a), breaks = CC[bottom, "t"], labels = xnames[bottom], 
                             sec.axis = sec_axis(trans ~ ., breaks = CC[top, "t"], labels = xnames[top]))
p2 = p2 + scale_y_continuous(limits = c(-a,a), breaks = CC[left, "t"], labels = xnames[left], 
                             sec.axis = sec_axis(trans ~ ., breaks = CC[right, "t"], labels = xnames[right]))  
p2 = p2 + coord_fixed()

p2 = p2 + theme_bw()  
p2 = p2 + theme(axis.line = element_line(colour = "black"),
                    panel.grid.major = element_blank(),
                    panel.grid.minor = element_blank(),
                    panel.border = element_blank(),
                    panel.background = element_blank()) 

p2
ggsave("~/surfdrive/multldm/mrmdu/paper/figures/liverplot2.pdf", plot = p2, width = 11.7, height = 8.3, units = "in", limitsize = FALSE)
```

# Multinomial Logistic Regression

As a comparison we also fit a standard multinomial logistic regression on this data set. Here is the R-function for minimizing the multinomial deviance again.

```{r, echo = FALSE}
multinomial.deviance <- function(pars,Y, X){
# this is a function to be used in the optim() function for the estimation of 
# a multinomial logistic regression
#
# pars: initial values of parameters pars0=matrix(0,p*J-1,1)
# Y: indicator matrix representing the response variable/classes
# X: matrix with values of the predictor variables- should include a vector of ones
#
# CALL: stats <- optim(pars,multinomial.deviance,NULL,Y,X,method="BFGS",control = list(trace=2, maxit=100),hessian=F)
# change to hessian = T if se's need to be computed. 
# 
# \copyright{M. de Rooij, 23-5-2013}
# -------------------------------------------------------------------
# -------------------------------------------------------------------
# extract the matrix B from the pars
B0 <- matrix(pars,ncol(X),ncol(Y)-1)
# first category is the baseline
B <- cbind(0,B0)
# or last category is baseline
# B <- cbind(B0,0)
# make the linear predictors and take exponent
exp.eta <- exp(X %*% B)
# compute the probabilities
sp = rowSums(exp.eta)
P = (1 / sp) * exp.eta
# compute the deviance
dev = -2 * sum(Y * log(P))
}
```

We use the first catgeory (PNN) as baseline and fit the model with the following code:

```{r}
pars0 = matrix(0,12,1) 
out.mlr <- optim(pars0, multinomial.deviance, NULL, Y = G, X = X, method="BFGS",control = list(trace=2, maxit=100), hessian=T)
```

The deviance is `r out.mlr$value` which is lower, but not substantially than the deviance of the ideal point model `r out$value`. In this case the number of parameters of the multinomial logistic regression model is 12, while for the ideal point model it is 11. 

```{r}
B = matrix(out.mlr$par,ncol(X),ncol(G)-1)
colnames(B) = c('AVH/PNN', 'PCH/PNN', 'ACH/PNN')
rownames(B) = c('Intercept', colnames(X)[-1])
knitr::kable(B, digits = 2, caption = 'Estimated parameter values')
```

Using the same procedure as before we can obtain standard errors, $z$-statistics and $p$-values for each of the parameter estimates. 

```{r}
SEs = sqrt(diag(solve(out.mlr$hessian)))
zstats = out.mlr$par/SEs
pvals = 1 - pnorm(abs(zstats))
TAB = cbind(out.mlr$par, SEs, zstats, pvals)
colnames(TAB) = c("estimates", "stand.error", "z", "p")
knitr::kable(TAB, digits = 2, caption = 'Estimates and Standard Errors')
```