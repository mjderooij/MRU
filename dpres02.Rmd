---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
title: "MRU: Analyses of Dutch Parliamentary Election data"
#thanks: "**Corresponding author**: rooijm@fsw.leidenuniv.nl"
author: Mark de Rooij
abstract: "Analysis of Dutch Parliamentary Election study data of 2002. There are five predictor variables: (E), Income Differences (ID), Asylum Seekers (AS), (C), and self left-right scaling (LR). The response variable is the vote in the 2002 election, it has originally 13 classes, but we removed classes with a frequency lower than 5, leaving 8 classes."
keywords: "Odds ratio, probability, visualization, ternary plot, trioscale"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
spacing: double
bibliography: ~/surfdrive/predictive-psychometrics/paper/predpsycho.bib
biblio-style: apsr
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/surfdrive/multldm/mrmdu")
library(lmdu)
source("~/surfdrive/Shared/MRU/examples/mru.extended.R")

library(nnet)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(GGally, quietly = TRUE, warn.conflicts = FALSE)
library(mvtnorm)
library(ggpubr)
library(metR)
library(tidyverse)
library(RColorBrewer)
library(jtools)
```

# Data



```{r}
load("~/surfdrive/LogitMDA/dpes02.Rdata")
mydat3= mydat2[, 14:19]
mydat3 = mydat3[complete.cases(mydat3), ]

X = mydat3[ , 1:5]; y = mydat3[, 6]
G = class.ind(y)
G = G[, c(1,2,3,4,5,7,9,10)]
y = y[which(rowSums(G) != 0)]
X = X[which(rowSums(G) != 0), ]
X = as.matrix(X)
X = X - outer(rep(1,nrow(X)), c(4,4,4,4,6))
G = G[which(rowSums(G) != 0), ]
colnames(G) = c("PvdA", "CDA", "VVD", "D66", "GL" , "CU", "LPF", "SP")

xnames = colnames(X)
ynames = colnames(G)
```


# Investigating Local optima

```{r}
set.seed(1234)

M1results = vector(mode = "list", length = 100)
M2results = vector(mode = "list", length = 100)
M3results = vector(mode = "list", length = 100)
M4results = vector(mode = "list", length = 100)
M5results = vector(mode = "list", length = 100)


M1results[[1]] = mru.da(X, G, m = 1)
M2results[[1]] = mru.da(X, G, m = 2)
M3results[[1]] = mru.da(X, G, m = 3)
M4results[[1]] = mru.da(X, G, m = 4)
M5results[[1]] = mru.da(X, G, m = 5)

for(r in 2:100){
  M1results[[r]] = mru.random( X, G, m = 1 )
  M2results[[r]] = mru.random( X, G, m = 2 )
  M3results[[r]] = mru.random( X, G, m = 3 )
  M4results[[r]] = mru.random( X, G, m = 4 )
  M5results[[r]] = mru.random( X, G, m = 5 )
}
save(M1results, M2results, M3results, M4results, M5results, file = "dpes02results.Rdata")
```

```{r}
library(ggplot2)
dev1 = rep(NA, 100)
dev2 = rep(NA, 100)
dev3 = rep(NA, 100)
dev4 = rep(NA, 100)
dev5 = rep(NA, 100)
for(r in 1:100){
  dev1[r] = M1results[[r]]$deviance
  dev2[r] = M2results[[r]]$deviance
  dev3[r] = M3results[[r]]$deviance
  dev4[r] = M4results[[r]]$deviance
  dev5[r] = M5results[[r]]$deviance
}
df = data.frame(repl = rep(1:100, 5), dimensionality = as.factor(rep(1:5, each = 100)), deviance = c(dev1, dev2, dev3, dev4, dev5))

p = ggplot(df, aes(dimensionality, deviance)) + geom_boxplot() + theme_apa()
ggsave("~/surfdrive/multldm/mrmdu/paper/figures/dpes02results.pdf", plot = p)
p
```



# Analysis with Distance Model


```{r}

out.dpes1 = M1results[[which.min(dev1)]]
out.dpes2 = M2results[[which.min(dev2)]]
out.dpes3 = M3results[[which.min(dev3)]]
out.dpes4 = M4results[[which.min(dev4)]]
out.dpes5 = M5results[[which.min(dev5)]]

fit = matrix(NA, 5, 4)
fit[ , 1] = c(1,2,3, 4, 5)
fit[1, 2] = out.dpes1$deviance
fit[2, 2] = out.dpes2$deviance
fit[3, 2] = out.dpes3$deviance
fit[4, 2] = out.dpes4$deviance
fit[5, 2] = out.dpes5$deviance
fit[1, 3] = ncol(X) * 1 + ncol(G) * 1 
fit[2, 3] = ncol(X) * 2 + ncol(G) * 2 - 2*1/2 
fit[3, 3] = ncol(X) * 3 + ncol(G) * 3 - 3*2/2
fit[4, 3] = ncol(X) * 4 + ncol(G) * 4 - 4*3/2
fit[5, 3] = ncol(X) * 5 + ncol(G) * 5 - 5*4/2
fit[, 4] = fit[, 2] + 2* fit[, 3]
colnames(fit) = c("Dimensionality", "Deviance", "#params", "AIC")
fit
```

# Variable Selection

One by one leave out the predictor variables
```{r}
BB = out.dpes2$B
VV = out.dpes2$V

out.dpes2.1 = mru.user(X[ , -1], G, m = 2, B.start = BB[-1, ], V.start = VV)
out.dpes2.2 = mru.user(X[ , -2], G, m = 2, B.start = BB[-2, ], V.start = VV)
out.dpes2.3 = mru.user(X[ , -3], G, m = 2, B.start = BB[-3, ], V.start = VV)
out.dpes2.4 = mru.da(X[ , -4], G, m = 2)
out.dpes2.5 = mru.da(X[ , -5], G, m = 2)


fit2 = matrix(NA, 6, 4)
fit2[ , 1] = c(0, 1, 2, 3, 4, 5)
fit2[1, 2] = out.dpes2$deviance
fit2[2, 2] = out.dpes2.1$deviance
fit2[3, 2] = out.dpes2.2$deviance
fit2[4, 2] = out.dpes2.3$deviance
fit2[5, 2] = out.dpes2.4$deviance
fit2[6, 2] = out.dpes2.5$deviance
fit2[1 , 3] = (ncol(X)) * 2 + ncol(G) * 2 - 2*1/2 
fit2[2:6 , 3] = (ncol(X) -1) * 2 + ncol(G) * 2 - 2*1/2 
fit2[ , 4] = fit2[, 2] + 2* fit2[, 3]
colnames(fit2) = c("Left out X", "Deviance", "#params", "AIC")
fit2
```

It seems all variables have a contribution.

# Visualization

Let us have a closer look at the two dimensional solution. 

```{r, echo = FALSE}
U = X %*% out.dpes2$B
V = out.dpes2$V
B = out.dpes2$B

B
V

Xo = X
P = nrow(B)
  
# for solid line
MCx1 <- data.frame(labs=character(),
                   varx = integer(),
                   Dim1 = double(),
                   Dim2 = double(), stringsAsFactors=FALSE)
# for markers
MCx2 <- data.frame(labs=character(),
                   varx = integer(),
                   Dim1 = double(),
                   Dim2 = double(), stringsAsFactors=FALSE)

ll = 0
lll = 0
for(pp in 1:P){
  b = matrix(B[pp , ], 2, 1)
  # solid line
  minx = min(Xo[, pp])
  maxx = max(Xo[, pp])
  m.x1 = c(minx,maxx)
  markers1 = matrix(m.x1, 2, 1) 
  markerscoord1 = outer(markers1, b) # markers1 %*% t(b %*% solve(t(b) %*% b))
  MCx1[(ll + 1): (ll + 2), 1] = paste0(c("min", "max"), pp)
  MCx1[(ll + 1): (ll + 2), 2] = pp
  MCx1[(ll + 1): (ll + 2), 3:4] = markerscoord1 
  ll = ll + 2
  # markers
  m.x2 = pretty(Xo[, pp]) 
  m.x2 = m.x2[which(m.x2 > minx & m.x2 < maxx)]
  l.m = length(m.x2)
  markers2 = matrix(m.x2, l.m, 1) 
  markerscoord2 = outer(markers2, b) # markers2 %*% t(b %*% solve(t(b) %*% b))
  MCx2[(lll + 1): (lll + l.m), 1] = paste(m.x2) 
  MCx2[(lll + 1): (lll + l.m), 2] = pp
  MCx2[(lll + 1): (lll + l.m), 3:4] = markerscoord2 
  lll = lll + l.m
} # loop p

NN = as.data.frame(U)
colnames(NN) = c("Dim1", "Dim2")

VV = as.data.frame(V)  
colnames(VV) = c("Dim1", "Dim2")

p = ggplot() +
    geom_point(data = VV, aes(x = Dim1, y = Dim2), colour = "darkgreen", size = 3) +
    geom_point(data = NN, aes(x = Dim1, y = Dim2, color = y), size = 1, show.legend = FALSE) + 
    xlab("Dimension 1") + 
    ylab("Dimension 2") + 
    #xlim(-6,6) + ylim(-6,6) +
    coord_fixed()   

p = p + geom_text(data = VV, aes(x = Dim1, y = Dim2), 
                  label = colnames(G),
                  vjust = 0, nudge_y = -0.5
                  )

xcol = "lightskyblue"
p = p + geom_abline(intercept = 0, slope = B[,2]/B[,1], colour = xcol, linetype = 3) +
  geom_line(data = MCx1, aes(x = Dim1, y = Dim2, group = varx), col = xcol, size = 1) + 
  geom_point(data = MCx2, aes(x = Dim1, y = Dim2), col = xcol) + 
  geom_text(data = MCx2, aes(x = Dim1, y = Dim2, label = labs), nudge_y = -0.08, size = 1.5)

a = ceiling(max(abs(c(ggplot_build(p)$layout$panel_scales_x[[1]]$range$range, ggplot_build(p)$layout$panel_scales_y[[1]]$range$range))))

idx1 = apply(abs(B), 1, which.max)
t = s = rep(NA,P)
for(pp in 1:P){
    t[(pp)] = (a * 1.1)/(abs(B[pp,idx1[(pp)]])) * B[pp,-idx1[(pp)]]
    s[(pp)] = sign(B[pp,idx1[(pp)]])
}
CC = cbind(idx1, t, s)
bottom = which(CC[, "idx1"] == 2 & CC[, "s"] == -1)
top =  which(CC[, "idx1"] == 2 & CC[, "s"] == 1)
right = which(CC[, "idx1"] == 1 & CC[, "s"] == 1)
left = which(CC[, "idx1"] == 1 & CC[, "s"] == -1)

p = p + scale_x_continuous(limits = c(-a,a), breaks = CC[bottom, "t"], labels = xnames[bottom], 
                             sec.axis = sec_axis(trans ~ ., breaks = CC[top, "t"], labels = xnames[top]))
p = p + scale_y_continuous(limits = c(-a,a), breaks = CC[left, "t"], labels = xnames[left], 
                             sec.axis = sec_axis(trans ~ ., breaks = CC[right, "t"], labels = xnames[right]))  

p = p + theme_bw() 

p = p + theme(axis.line = element_line(colour = "black"),
                    panel.grid.major = element_blank(),
                    panel.grid.minor = element_blank(),
                    panel.border = element_blank(),
                    panel.background = element_blank()) 

p
ggsave("~/surfdrive/multldm/mrmdu/paper/figures/dpesplot1.pdf", plot = p, width = 11.7, height = 8.3, units = "in", limitsize = FALSE)
```

Let us have a further look at the classification performance
```{r}
D = outer(diag(U %*% t(U)), rep(1, 8)) + outer(rep(1,275), diag(V %*% t(V))) - 2* U %*% t(V)
D = sqrt(D)
PR = exp(-D)/rowSums(exp(-D))
yhat = apply(PR, 1, which.max)
yy = apply(G, 1, which.max)
tab1 = table(yy, yhat)
tab1
```

In total, `r 100*sum(diag(tab1))/sum(tab1)` percent is correctly classified 

# Simulation

First develop a function that generates data based on population class points, regression weights and some covariance matrix:

```{r}
gendata = function(N, covX, B, V){
  library(mvtnorm)
  library(poLCA)
  P = nrow(B)
  C = nrow(V)
  X = rmvnorm(N, mean = rep(0, P), sigma = covX)
  X = scale(X, center = TRUE, scale = FALSE)
  U = X %*% B
  
  D = sqrt(outer(diag(U %*% t(U)), rep(1, C)) + outer(rep(1, N), diag(V %*% t(V))) - 2 * U %*% t(V))
  Pr = exp(-D)/rowSums(exp(-D))
  G = class.ind(rmulti(Pr))
  output = list(X = X, G = G)
}

mse = function(A, B){(A-B)^2}

```

And now we simulate data with these parameters and varying sample sizes (100, 200, 500, 1000). On the generated data sets we fit the multinomial restricted unfolding and we compare the obtained results with the population parameters.

```{r}
V = out.dpes2$V
B = out.dpes2$B
covX = cov(X)

# small simulation
Ns = c(100, 200, 500, 1000)
plts = vector(mode = "list", length = length(Ns))
Bbias = vector(mode = "list", length = length(Ns))
Brmse = vector(mode = "list", length = length(Ns))
Vbias = vector(mode = "list", length = length(Ns))
Vrmse = vector(mode = "list", length = length(Ns))

source("ggbagplot.R")
set.seed(1234)
for(n in 1:length(Ns)){
  N = Ns[n]
  Bs = vector(mode = "list", length = 100)
  Vs = vector(mode = "list", length = 100)
  
  
  for(rep in 1:100){
    #cat("This is repetition:", rep, "for sample size", n ,"\n")
    mydat = gendata(N, covX, B, V)
    if(ncol(mydat$G) != 8){mydat = gendata(N, covX, B, V)}
    out <- mru.user(X = mydat$X, G = mydat$G, m = 2, B.start = B, V.start = V)
    # orthogonal procrustes analysis on U
    pq = svd(t(V) %*% out$V)
    TT = pq$v %*% t(pq$u)
    Bs[[rep]] = out$B %*% TT
    Vs[[rep]] = out$V %*% TT
  }
  
  # # bias
  Bbias[[n]] = Reduce("+", Bs) / length(Bs) - B
  Vbias[[n]] = Reduce("+", Vs) / length(Vs) - V
  # 
  # rmse
  Brmse[[n]] = sqrt(Reduce("+" ,lapply(Bs, mse, B))/length(Bs))
  Vrmse[[n]] = sqrt(Reduce("+" ,lapply(Vs, mse, V))/length(Vs))
  
  ####################################################
  ####################################################
  # A visualization of the results
  ####################################################
  ####################################################
  
  Bdf = data.frame(B); colnames(Bdf) = c("x", "y")
  Vdf = data.frame(V); colnames(Vdf) = c("x", "y")
  Vdf$class = as.factor(c(1,2,3,4, 5, 6, 7, 8))
  
  # class points
  Vslong = matrix(NA, 800, 2); Bslong = matrix(NA, 500, 2)
  for(r in 1:100){
    Vslong[((r-1)*8 + 1):(r*8), ] = Vs[[r]]
    Bslong[((r-1)*5 + 1):(r*5), ] = Bs[[r]]
  }

  Vslong = data.frame(cbind(rep(1:8, 100), Vslong))
  colnames(Vslong) = c("class", "x", "y")
  Vslong$class = as.factor(Vslong$class)

  hull_data <-
    Vslong %>%
    group_by(class) %>%
    slice(chull(x, y))

  # plt = ggplot(Vslong, aes(x = x, y = y, col = class)) +
  #   # xlim(-15,15) + 
  #   # ylim(-15,15) +
  #   geom_point(alpha = 0.5) + 
  #   geom_polygon(data = hull_data, aes(fill = class, colour = class), alpha = 0.3, show.legend = FALSE) + 
  #   scale_color_manual(values = brewer.pal(9,"Greens")[6:9]) +
  #   scale_fill_manual(values = brewer.pal(9,"Greens")[6:9]) 
  
  plt = ggplot(Vslong, aes(x = x, y = y, col = class)) +
    geom_point(alpha = 0.5) + 
    geom_bag(prop = 0.9, aes(fill = class, colour = class), alpha = 0.3, show.legend = FALSE) +
    scale_color_manual(values = brewer.pal(8,"Paired")[1:8]) +
    scale_fill_manual(values = brewer.pal(8,"Paired")[1:8]) +
    # scale_color_manual(values = brewer.pal(9,"Greens")[2:9]) +
    # scale_fill_manual(values = brewer.pal(9,"Greens")[2:9]) +
    theme(legend.position = "none")
  
  # predictor variables
  Bslong = data.frame(cbind(rep(1:5, 100), Bslong))
  colnames(Bslong) = c("pred", "x", "y")
  Bslong$pred = as.factor(Bslong$pred)
  
  plt = plt + 
    #geom_abline(intercept = 0, slope = Bslong$y/Bslong$x, col = "lightskyblue", alpha = 0.3) + 
    geom_point(data = Bslong, aes(x = x, y = y), colour = "darkblue", alpha = 0.3)
  
  plt = plt + geom_point(data = Vdf, aes(x = x, y = y), colour = brewer.pal(8,"Paired")[1:8], 
                         shape = 18, size = 5) +
    geom_abline(intercept = 0, slope = Bdf[ ,2]/Bdf[ ,1], colour = "darkblue", size = 1) + 
    geom_point(data = Bdf, aes(x =x, y = y), col = "darkblue", size = 5)

  plt = plt + 
    labs(
      x = "Dimension 1",
      y = "Dimension 2"
      ) + 
    xlim(-12,12) + 
    ylim(-12,12) +
    coord_fixed() +
    theme_apa() + 
    theme(legend.position = "none") 

  # for(r in 1:100){
  #   Bdfr = data.frame(Bs[[r]]); colnames(Bdfr) = c("x", "y")
  #   Vdfr = data.frame(Vs[[r]]); colnames(Vdfr) = c("x", "y")
  #   plt = plt + geom_point(data = Bdfr, aes(x = x, y = y), col = "darkblue", size = 0.2, alpha = 0.1) + 
  #     geom_point(data = Vdfr, aes(x = x,y = y), col = "darkred", size = 0.2, alpha = 0.1)
  # }
  
  plts[[n]] = plt + labs(title = paste("Estimates for N = ", N))
}

Bbias
Brmse

Vbias
Vrmse

save(Bbias, Brmse, Vbias, Vrmse, file = "simdpesresults.Rdata")
plt = ggarrange(plts[[1]], plts[[2]], plts[[3]], plts[[4]], nrow = 2, ncol = 2)  
plt

ggsave("~/surfdrive/multldm/mrmdu/paper/figures/simdpes.pdf", plot = plt, width = 11.7, height = 8.3, units = "in", limitsize = FALSE)
```

# Analysis with Squared Distance Model

Identification restrictions

- Translation: set the coordinates for first class to zero
- Rotation: set the upper triangle of \bf{V} to zero
- Scaling: set some elements in \bf{V} to one

The following function minimizes the deviance for the IPC model. 

```{r, echo = FALSE}
ipc.deviance <- function(pars, G, X){
  # Deviance of IPC model for LIVER data
  # pars: initial values of parameters
  # G: indicator matrix representing the response classes(n  by J)
  # X: predictor variables (n by p + 1); should include a vector of ones
  # CALL: stats <- optim(pars, ipc.deviance2, NULL, Y, X, Z, method ="BFGS", 
  #                      control = list(trace = 2, maxit = 100), hessian = F)
  # Change to hessian = T if se's need to be computed.
  # \copyright M. de Rooij, 23-5-2013                 \
  # ------------------------------------------------------------------
  # extract matrix B from the pars
  I = nrow(G)
  C = ncol(G)
  p = ncol(X)
  B = matrix(pars[1:(2*p)], p, 2)
  # create Z - matrix with coordinates of class points
  # TRANSLATION, SCALING, AND ROTATION
  V = matrix(0, C, 2)
  #V[2,1] = V[3,2] = 1
  V[2:C,1] = pars[(2*p+1):(2*p+7)]
  V[3:C,2] = pars[(2*p+8):(2*p+13)]

  # Make the linear predictors and compute the squred distances
  N = X %*% B
  
  v2 = rowSums(V ^ 2)
  D =  - 2 * N %*% t(V) + rep(1, I) %o% v2  
  
  # Compute probabilities and deviance
  P = exp(-D)
  sp = rowSums(P)
  P = (1 / sp) * P
  dev = -2 * sum(G * log(P))
}
```

With the following code we first create our design matrix and initiate parameters for minimization of the deviance function. Then we call the \texttt{optim}-function to find a set of parameters that maximizes the likelihood or minimizes the deviance. 

```{r}
da.out = mru.start(X, G, m = 2, start = "da")
X = cbind(1,X)
#pars0 = c(rep(0,12), rnorm(13))
pars0 = c(0, da.out$B[,1], 0, da.out$B[,2], da.out$V[2:8, 1], da.out$V[3:8, 2])
out = optim(pars0,ipc.deviance, G = G, X = X, method = "BFGS", control = list(trace = 2, maxit = 200), hessian = F)
```

The value of the deviance is `r out$value` for which the estimated parameters are
```{r}
P = ncol(X)
C = ncol(G)

B = matrix(out$par[1:(2*P)], P, 2)
B

U = X %*% B

V = matrix(0, C, 2)
V[2:C,1] = out$par[(2*P+1):(2*P+7)]
V[3:C,2] = out$par[(2*P+8):(2*P+13)]

V

```


Let us have a further look at the classification performance
```{r}
D2 = outer(diag(U %*% t(U)), rep(1, 8)) + outer(rep(1,275), diag(V %*% t(V))) - 2* U %*% t(V)
PR2 = exp(-D2)/rowSums(exp(-D2))
yhat2 = apply(PR2, 1, which.max)
tab2 = table(yy, yhat2)
tab2
```

In total, `r 100*sum(diag(tab2))/sum(tab2)` percent is correctly classified 

## Graphical representation

```{r, echo = FALSE}
# translation such that U is centered
VV = V - outer(rep(1,8), B[1, ])
# rotation to principal axes
UU = X[, -1] %*% B[-1, ]
R = eigen(t(UU)%*%UU)$vectors
B = B[-1, ] %*% R
UU = UU %*% R
VV = VV %*% R
```

Let us have a further look at the classification performance
```{r}
D2 = outer(diag(UU %*% t(UU)), rep(1, 8)) + outer(rep(1,275), diag(VV %*% t(VV))) - 2* UU %*% t(VV)
PR2 = exp(-D2)/rowSums(exp(-D2))
yhat2 = apply(PR2, 1, which.max)
tab2 = table(yy, yhat2)
tab2
```

In total, `r 100*sum(diag(tab2))/sum(tab2)` percent is correctly classified 


```{r}
NN = as.data.frame(UU)
colnames(NN) = c("Dim1", "Dim2")

VV = as.data.frame(V)  
colnames(VV) = c("Dim1", "Dim2")
P = nrow(B)
Xo = X[, -1]

# for solid line
MCx1 <- data.frame(labs=character(),
                   varx = integer(),
                   Dim1 = double(),
                   Dim2 = double(), stringsAsFactors=FALSE)
# for markers
MCx2 <- data.frame(labs=character(),
                   varx = integer(),
                   Dim1 = double(),
                   Dim2 = double(), stringsAsFactors=FALSE)

ll = 0
lll = 0
for(pp in 1:P){
  b = matrix(B[pp , ], 2, 1)
  # solid line
  minx = min(Xo[, pp])
  maxx = max(Xo[, pp])
  m.x1 = c(minx,maxx)
  markers1 = matrix(m.x1, 2, 1) 
  markerscoord1 = outer(markers1, b) # markers1 %*% t(b %*% solve(t(b) %*% b))
  MCx1[(ll + 1): (ll + 2), 1] = paste0(c("min", "max"), pp)
  MCx1[(ll + 1): (ll + 2), 2] = pp
  MCx1[(ll + 1): (ll + 2), 3:4] = markerscoord1 
  ll = ll + 2
  # markers
  m.x2 = pretty(Xo[, pp]) 
  m.x2 = m.x2[which(m.x2 > minx & m.x2 < maxx)]
  l.m = length(m.x2)
  markers2 = matrix(m.x2, l.m, 1) 
  markerscoord2 = outer(markers2, b) # markers2 %*% t(b %*% solve(t(b) %*% b))
  MCx2[(lll + 1): (lll + l.m), 1] = paste(m.x2) 
  MCx2[(lll + 1): (lll + l.m), 2] = pp
  MCx2[(lll + 1): (lll + l.m), 3:4] = markerscoord2 
  lll = lll + l.m
} # loop p

p2 = ggplot() +
    geom_point(data = VV, aes(x = Dim1, y = Dim2), colour = "darkgreen", size = 3) +
    geom_point(data = NN, aes(x = Dim1, y = Dim2, color = y), size = 1, show.legend = FALSE) + 
    xlab("Dimension 1") + 
    ylab("Dimension 2") 

p2 = p2 + geom_text(data = VV, aes(x = Dim1, y = Dim2), 
                  label = colnames(G),
                  vjust = 0, nudge_y = -0.5)

xcol = "lightskyblue"
p2 = p2 + geom_abline(intercept = 0, slope = B[,2]/B[,1], colour = xcol, linetype = 3) +
    geom_line(data = MCx1, aes(x = Dim1, y = Dim2, group = varx), col = xcol, size = 1) + 
    geom_point(data = MCx2, aes(x = Dim1, y = Dim2), col = xcol) + 
    geom_text(data = MCx2, aes(x = Dim1, y = Dim2, label = labs), nudge_y = -0.08, size = 1.5)

a = ceiling(max(abs(c(ggplot_build(p2)$layout$panel_scales_x[[1]]$range$range, ggplot_build(p2)$layout$panel_scales_y[[1]]$range$range))))

idx1 = apply(abs(B), 1, which.max)
t = s = rep(NA,(P))
for(pp in 1:(P)){
    t[(pp)] = (a *1.1)/(abs(B[pp,idx1[(pp)]])) * B[pp,-idx1[(pp)]]
    s[(pp)] = sign(B[pp,idx1[(pp)]])
}
CC = cbind(idx1, t, s)
bottom = which(CC[, "idx1"] == 2 & CC[, "s"] == -1)
top =  which(CC[, "idx1"] == 2 & CC[, "s"] == 1)
right = which(CC[, "idx1"] == 1 & CC[, "s"] == 1)
left = which(CC[, "idx1"] == 1 & CC[, "s"] == -1)

p2 = p2 + scale_x_continuous(limits = c(-a,a), breaks = CC[bottom, "t"], labels = xnames[bottom], 
                             sec.axis = sec_axis(trans ~ ., breaks = CC[top, "t"], labels = xnames[top]))
p2 = p2 + scale_y_continuous(limits = c(-a,a), breaks = CC[left, "t"], labels = xnames[left], 
                             sec.axis = sec_axis(trans ~ ., breaks = CC[right, "t"], labels = xnames[right]))  
p2 = p2 + coord_fixed()

p2 = p2 + theme_bw() 
p2 = p2 + theme(axis.line = element_line(colour = "black"),
                    panel.grid.major = element_blank(),
                    panel.grid.minor = element_blank(),
                    panel.border = element_blank(),
                    panel.background = element_blank()) 

 
p2
ggsave("~/surfdrive/multldm/mrmdu/paper/figures/dpesplot2.pdf", plot = p2, width = 11.7, height = 8.3, units = "in", limitsize = FALSE)
```

# Multinomial Logistic Regression

As a comparison we also fit a standard multinomial logistic regression on this data set. Here is the R-function for minimizing the multinomial deviance again.

```{r, echo = FALSE}
multinomial.deviance <- function(pars,Y, X){
# this is a function to be used in the optim() function for the estimation of 
# a multinomial logistic regression
#
# pars: initial values of parameters pars0=matrix(0,p*J-1,1)
# Y: indicator matrix representing the response variable/classes
# X: matrix with values of the predictor variables- should include a vector of ones
#
# CALL: stats <- optim(pars,multinomial.deviance,NULL,Y,X,method="BFGS",control = list(trace=2, maxit=100),hessian=F)
# change to hessian = T if se's need to be computed. 
# 
# \copyright{M. de Rooij, 23-5-2013}
# -------------------------------------------------------------------
# -------------------------------------------------------------------
# extract the matrix B from the pars
B0 <- matrix(pars,ncol(X),ncol(Y)-1)
# first category is the baseline
B <- cbind(0,B0)
# or last category is baseline
# B <- cbind(B0,0)
# make the linear predictors and take exponent
exp.eta <- exp(X %*% B)
# compute the probabilities
sp = rowSums(exp.eta)
P = (1 / sp) * exp.eta
# compute the deviance
dev = -2 * sum(Y * log(P))
}
```

We use the first catgeory as baseline and fit the model with the following code:

```{r}
pars0 = matrix(0,42,1) 
out.mlr <- optim(pars0, multinomial.deviance, NULL, Y = G, X = X, method="BFGS",control = list(trace=2, maxit=100), hessian=T)
```

The deviance is `r out.mlr$value` which is lower, but not substantially than the deviance of the ideal point model `r out$value`. In this case the number of parameters of the multinomial logistic regression model is 12, while for the ideal point model it is 11. 

```{r}
B = matrix(out.mlr$par,ncol(X),ncol(G)-1)
colnames(B) = c('CDA/PvdA', 'VVD/PvdA', 'D66/PvdA', "GL/PvdA", "CU/PvdA", "LPF/PvdA", "SP/PvdA")
rownames(B) = c('Intercept', colnames(X)[-1])
knitr::kable(B, digits = 2, caption = 'Estimated parameter values')
```

Using the same procedure as before we can obtain standard errors, $z$-statistics and $p$-values for each of the parameter estimates. 

```{r}
SEs = sqrt(diag(solve(out.mlr$hessian)))
zstats = out.mlr$par/SEs
pvals = 1 - pnorm(abs(zstats))
TAB = cbind(out.mlr$par, SEs, zstats, pvals)
colnames(TAB) = c("estimates", "stand.error", "z", "p")
knitr::kable(TAB, digits = 2, caption = 'Estimates and Standard Errors')
```